{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2236708,"sourceType":"datasetVersion","datasetId":1343913}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nimport sys\nimport argparse\nimport matplotlib.pyplot as plt\nplt.rcParams[\"axes.grid\"] = False\nimport matplotlib.image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport os\nimport gzip\nimport struct\nimport array\nfrom urllib.request import urlretrieve\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.optim import Adam\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T01:07:44.900858Z","iopub.execute_input":"2024-04-22T01:07:44.901395Z","iopub.status.idle":"2024-04-22T01:07:48.842488Z","shell.execute_reply.started":"2024-04-22T01:07:44.901362Z","shell.execute_reply":"2024-04-22T01:07:48.841260Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T08:16:25.670868Z","iopub.execute_input":"2024-04-21T08:16:25.671745Z","iopub.status.idle":"2024-04-21T08:16:25.678839Z","shell.execute_reply.started":"2024-04-21T08:16:25.671709Z","shell.execute_reply":"2024-04-21T08:16:25.677725Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Loading the data","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=4)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T04:08:43.716000Z","iopub.execute_input":"2024-04-21T04:08:43.716415Z","iopub.status.idle":"2024-04-21T04:08:43.727605Z","shell.execute_reply.started":"2024-04-21T04:08:43.716380Z","shell.execute_reply":"2024-04-21T04:08:43.725763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def array_to_image(array):\n    return np.reshape(np.array(array), [28, 28])\n\ndef concat_images(images, row, col, padding=3):\n    result = np.zeros((28 * row + (row - 1) * padding, 28 * col + (col - 1) * padding))\n    for i in range(row):\n        for j in range(col):\n            result[i * 28 + (i * padding):i * 28 + (i * padding) + 28,\n                   j * 28 + (j * padding):j * 28 + (j * padding) + 28] = images[i + j * row]\n    return result\n\nclass ConditionalEncoder(nn.Module):\n    def __init__(self, latent_dimension, hidden_units, data_dimension, num_classes):\n        super(ConditionalEncoder, self).__init__()\n        self.fc1 = nn.Linear(data_dimension + num_classes, hidden_units)\n        self.bn1 = nn.BatchNorm1d(hidden_units)\n        self.fc2_mu = nn.Linear(hidden_units, latent_dimension)\n        self.fc2_sigma = nn.Linear(hidden_units, latent_dimension)\n\n    def forward(self, x, labels):\n        x = torch.cat([x, labels.float()], dim=1)\n        hidden = self.bn1(F.tanh(self.fc1(x)))\n        mu = self.fc2_mu(hidden)\n        log_sigma_square = self.fc2_sigma(hidden)\n        sigma_square = torch.exp(torch.clamp(log_sigma_square, max=10))\n        return mu, sigma_square\n\nclass ConditionalDecoder(nn.Module):\n    def __init__(self, latent_dimension, data_dimension, hidden_units, num_classes):\n        super(ConditionalDecoder, self).__init__()\n        self.fc1_dec = nn.Linear(latent_dimension + num_classes, hidden_units)\n        self.fc2_dec = nn.Linear(hidden_units, data_dimension)\n\n    def forward(self, z, labels):\n        z = torch.cat([z, labels.float()], dim=1)\n        hidden_dec = F.tanh(self.fc1_dec(z))\n        p = torch.sigmoid(self.fc2_dec(hidden_dec))\n        return p\n\nclass ConditionalVAE(nn.Module):\n    def __init__(self, args):\n        super(ConditionalVAE, self).__init__()\n        self.encoder = ConditionalEncoder(args.latent_dimension, args.hidden_units, args.data_dimension, args.num_classes)\n        self.decoder = ConditionalDecoder(args.latent_dimension, args.data_dimension, args.hidden_units, args.num_classes)\n        self.e_path = args.e_path\n        self.d_path = args.d_path\n        if args.resume_training:\n            self.load_state_dict(torch.load(self.e_path))\n\n    @staticmethod\n    def sample_diagonal_gaussian(mu, sigma_square):\n        sigma = torch.sqrt(sigma_square)\n        return mu + torch.randn_like(mu) * sigma\n\n    @staticmethod\n    def logpdf_diagonal_gaussian(z, mu, sigma_square):\n        sigma_square = torch.clamp(sigma_square, min=1e-6)\n        covariance_matrix = torch.diag_embed(sigma_square)\n        dist = MultivariateNormal(mu, covariance_matrix)\n        return dist.log_prob(z)\n\n    @staticmethod\n    def logpdf_bernoulli(x, p):\n        return (x * torch.log(p + 1e-8) + (1 - x) * torch.log(1 - p + 1e-8)).sum(dim=1)\n\n    def elbo_loss(self, sampled_z, mu, sigma_square, x, p):\n        log_q = self.logpdf_diagonal_gaussian(sampled_z, mu, sigma_square)\n        z_mu = torch.zeros_like(mu)\n        z_sigma = torch.ones_like(sigma_square)\n        log_p_z = self.logpdf_diagonal_gaussian(sampled_z, z_mu, z_sigma)\n        log_p = self.logpdf_bernoulli(x, p)\n        return (log_p + log_p_z - log_q).mean()\n\n    def train_model(self, train_loader, val_loader, epochs=200, save_interval=10):\n        optimizer = Adam(self.parameters(), lr=0.001)\n        for epoch in range(epochs):\n            self.train()\n            train_loss = 0\n            for data, labels in train_loader:\n                data = data.view(data.size(0), -1)\n                labels = F.one_hot(labels, num_classes=2)  # Ensure labels are one-hot encoded\n                optimizer.zero_grad()\n                mu, sigma_square = self.encoder(data, labels)\n                zs = self.sample_diagonal_gaussian(mu, sigma_square)\n                p = self.decoder(zs, labels)\n                loss = -self.elbo_loss(zs, mu, sigma_square, data, p)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n                optimizer.step()\n                train_loss += loss.item()\n            train_loss /= len(train_loader.dataset)\n            print(f'Epoch {epoch+1}, Average Training Loss: {train_loss:.4f}')\n            if epoch % save_interval == 0:\n                self.evaluate(val_loader)\n                torch.save(self.state_dict(), f'model_epoch_{epoch}.pth')\n\n    def evaluate(self, loader):\n        self.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for data, labels in loader:\n                data = data.view(data.size(0), -1)\n                labels = F.one_hot(labels, num_classes=2)  # Ensure labels are one-hot encoded\n                mu, sigma_square = self.encoder(data, labels)\n                zs = self.sample_diagonal_gaussian(mu, sigma_square)\n                p = self.decoder(zs, labels)\n                loss = -self.elbo_loss(zs, mu, sigma_square, data, p)\n                val_loss += loss.item()\n        val_loss /= len(loader.dataset)\n        print(f'Validation Loss: {val_loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:33:20.434533Z","iopub.execute_input":"2024-04-21T21:33:20.434943Z","iopub.status.idle":"2024-04-21T21:33:20.461023Z","shell.execute_reply.started":"2024-04-21T21:33:20.434909Z","shell.execute_reply":"2024-04-21T21:33:20.459696Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),  # Convert image to grayscale\n    transforms.Resize((128, 128)),                  # Resize images to 28x28\n    transforms.ToTensor(),                        # Convert images to PyTorch tensors\n    transforms.Normalize((0.5,), (0.5,)),         # Normalize tensors\n    transforms.Lambda(lambda x: x.view(-1))       # Flatten the tensors from [1, 28, 28] to [784]\n])\n\n# Load datasets\ndataset_path = '/kaggle/input/brian-tumor-dataset/Brain Tumor Data Set/Brain Tumor Data Set/'\ndataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)\n\n# Split dataset into train and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n# Define DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n# Optionally, check the output shape\nimages, labels = next(iter(train_loader))\nprint(\"Image batch shape:\", images.shape)  # Expected output: Image batch shape: [64, 784]\nprint(\"Label batch shape:\", labels.shape)  #\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T01:08:01.921732Z","iopub.execute_input":"2024-04-22T01:08:01.922378Z","iopub.status.idle":"2024-04-22T01:08:07.988122Z","shell.execute_reply.started":"2024-04-22T01:08:01.922341Z","shell.execute_reply":"2024-04-22T01:08:07.986111Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([64, 16384])\nLabel batch shape: torch.Size([64])\n","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=\"VAE Model Configuration\")\n    parser.add_argument('--latent_dimension', type=int, default=2, help='Dimensionality of the latent space')\n    parser.add_argument('--hidden_units', type=int, default=800, help='Number of units in the hidden layer of the encoder and decoder')\n    parser.add_argument('--data_dimension', type=int, default=128*128, help='Dimensionality of the flattened input images (e.g., 28*28 for MNIST)')\n    parser.add_argument('--batch_size', type=int, default=100, help='Training batch size')\n    parser.add_argument('--num_epochs', type=int, default=45, help='Number of epochs to train')\n    parser.add_argument('--e_path', type=str, default='encoder.pth', help='Path to save the encoder weights')\n    parser.add_argument('--d_path', type=str, default='decoder.pth', help='Path to save the decoder weights')\n    parser.add_argument('--resume_training', type=bool, default=False, help='Flag to determine if training should be resumed from saved model')\n    parser.add_argument('--num_classes', type=int, default=2)\n    return parser.parse_args(args=[])  # Here, args=[] is used for illustration in notebooks or scripts without command line args\n\nargs = get_args()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:03:06.908359Z","iopub.execute_input":"2024-04-21T21:03:06.908760Z","iopub.status.idle":"2024-04-21T21:03:06.917817Z","shell.execute_reply.started":"2024-04-21T21:03:06.908729Z","shell.execute_reply":"2024-04-21T21:03:06.916518Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vae_model = ConditionalVAE(args)  # make sure to define args or pass relevant parameters\nvae_model.train_model(train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:35:02.755753Z","iopub.execute_input":"2024-04-21T21:35:02.756795Z","iopub.status.idle":"2024-04-21T23:17:46.875503Z","shell.execute_reply.started":"2024-04-21T21:35:02.756760Z","shell.execute_reply":"2024-04-21T23:17:46.874338Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1, Average Training Loss: -884.8233\nValidation Loss: -2139.4828\nEpoch 2, Average Training Loss: -2149.3870\nEpoch 3, Average Training Loss: -2342.2513\nEpoch 4, Average Training Loss: -2407.0156\nEpoch 5, Average Training Loss: -2475.7737\nEpoch 6, Average Training Loss: -2479.3791\nEpoch 7, Average Training Loss: -2488.1414\nEpoch 8, Average Training Loss: -2487.5270\nEpoch 9, Average Training Loss: -2489.3360\nEpoch 10, Average Training Loss: -2493.8493\nEpoch 11, Average Training Loss: -2495.6888\nValidation Loss: -2603.2906\nEpoch 12, Average Training Loss: -2493.7324\nEpoch 13, Average Training Loss: -2495.7661\nEpoch 14, Average Training Loss: -2495.8654\nEpoch 15, Average Training Loss: -2497.8038\nEpoch 16, Average Training Loss: -2500.7753\nEpoch 17, Average Training Loss: -2495.9463\nEpoch 18, Average Training Loss: -2498.2666\nEpoch 19, Average Training Loss: -2501.1379\nEpoch 20, Average Training Loss: -2499.8363\nEpoch 21, Average Training Loss: -2503.0493\nValidation Loss: -2606.6601\nEpoch 22, Average Training Loss: -2502.4257\nEpoch 23, Average Training Loss: -2501.3962\nEpoch 24, Average Training Loss: -2500.2914\nEpoch 25, Average Training Loss: -2501.8164\nEpoch 26, Average Training Loss: -2501.6622\nEpoch 27, Average Training Loss: -2502.7008\nEpoch 28, Average Training Loss: -2501.5283\nEpoch 29, Average Training Loss: -2503.5747\nEpoch 30, Average Training Loss: -2501.8134\nEpoch 31, Average Training Loss: -2501.5132\nValidation Loss: -2608.3318\nEpoch 32, Average Training Loss: -2504.0553\nEpoch 33, Average Training Loss: -2501.9227\nEpoch 34, Average Training Loss: -2505.2946\nEpoch 35, Average Training Loss: -2502.3965\nEpoch 36, Average Training Loss: -2500.6975\nEpoch 37, Average Training Loss: -2503.6126\nEpoch 38, Average Training Loss: -2503.1725\nEpoch 39, Average Training Loss: -2502.0585\nEpoch 40, Average Training Loss: -2504.2876\nEpoch 41, Average Training Loss: -2503.9830\nValidation Loss: -2609.5073\nEpoch 42, Average Training Loss: -2507.5356\nEpoch 43, Average Training Loss: -2503.8870\nEpoch 44, Average Training Loss: -2502.4635\nEpoch 45, Average Training Loss: -2503.8014\nEpoch 46, Average Training Loss: -2504.9624\nEpoch 47, Average Training Loss: -2499.6125\nEpoch 48, Average Training Loss: -2505.6990\nEpoch 49, Average Training Loss: -2505.6804\nEpoch 50, Average Training Loss: -2506.9084\nEpoch 51, Average Training Loss: -2503.6387\nValidation Loss: -2608.2081\nEpoch 52, Average Training Loss: -2503.1586\nEpoch 53, Average Training Loss: -2504.1953\nEpoch 54, Average Training Loss: -2503.9172\nEpoch 55, Average Training Loss: -2504.0065\nEpoch 56, Average Training Loss: -2504.6244\nEpoch 57, Average Training Loss: -2506.6470\nEpoch 58, Average Training Loss: -2507.3923\nEpoch 59, Average Training Loss: -2504.4550\nEpoch 60, Average Training Loss: -2505.5959\nEpoch 61, Average Training Loss: -2505.5589\nValidation Loss: -2611.1827\nEpoch 62, Average Training Loss: -2504.6934\nEpoch 63, Average Training Loss: -2505.8437\nEpoch 64, Average Training Loss: -2504.9781\nEpoch 65, Average Training Loss: -2504.9200\nEpoch 66, Average Training Loss: -2507.0678\nEpoch 67, Average Training Loss: -2506.1359\nEpoch 68, Average Training Loss: -2505.5685\nEpoch 69, Average Training Loss: -2504.7482\nEpoch 70, Average Training Loss: -2504.1683\nEpoch 71, Average Training Loss: -2508.3428\nValidation Loss: -2608.8237\nEpoch 72, Average Training Loss: -2504.2223\nEpoch 73, Average Training Loss: -2507.3767\nEpoch 74, Average Training Loss: -2506.6534\nEpoch 75, Average Training Loss: -2505.3107\nEpoch 76, Average Training Loss: -2506.5528\nEpoch 77, Average Training Loss: -2504.4628\nEpoch 78, Average Training Loss: -2505.5533\nEpoch 79, Average Training Loss: -2507.2509\nEpoch 80, Average Training Loss: -2507.3078\nEpoch 81, Average Training Loss: -2507.4390\nValidation Loss: -2607.4946\nEpoch 82, Average Training Loss: -2505.4100\nEpoch 83, Average Training Loss: -2509.7103\nEpoch 84, Average Training Loss: -2506.3613\nEpoch 85, Average Training Loss: -2509.2307\nEpoch 86, Average Training Loss: -2506.8421\nEpoch 87, Average Training Loss: -2505.1098\nEpoch 88, Average Training Loss: -2509.5566\nEpoch 89, Average Training Loss: -2506.8597\nEpoch 90, Average Training Loss: -2509.5390\nEpoch 91, Average Training Loss: -2507.5276\nValidation Loss: -2610.8008\nEpoch 92, Average Training Loss: -2507.8201\nEpoch 93, Average Training Loss: -2507.6678\nEpoch 94, Average Training Loss: -2510.5261\nEpoch 95, Average Training Loss: -2509.3526\nEpoch 96, Average Training Loss: -2512.3791\nEpoch 97, Average Training Loss: -2509.1651\nEpoch 98, Average Training Loss: -2511.8600\nEpoch 99, Average Training Loss: -2511.2886\nEpoch 100, Average Training Loss: -2510.8552\nEpoch 101, Average Training Loss: -2512.0208\nValidation Loss: -2614.9838\nEpoch 102, Average Training Loss: -2513.0677\nEpoch 103, Average Training Loss: -2511.5696\nEpoch 104, Average Training Loss: -2511.5935\nEpoch 105, Average Training Loss: -2511.4753\nEpoch 106, Average Training Loss: -2514.0715\nEpoch 107, Average Training Loss: -2512.5257\nEpoch 108, Average Training Loss: -2514.0156\nEpoch 109, Average Training Loss: -2513.0696\nEpoch 110, Average Training Loss: -2515.0565\nEpoch 111, Average Training Loss: -2512.1381\nValidation Loss: -2617.5934\nEpoch 112, Average Training Loss: -2513.2247\nEpoch 113, Average Training Loss: -2513.8892\nEpoch 114, Average Training Loss: -2514.5048\nEpoch 115, Average Training Loss: -2513.9926\nEpoch 116, Average Training Loss: -2514.2608\nEpoch 117, Average Training Loss: -2513.1477\nEpoch 118, Average Training Loss: -2513.1112\nEpoch 119, Average Training Loss: -2512.8019\nEpoch 120, Average Training Loss: -2515.4580\nEpoch 121, Average Training Loss: -2511.8635\nValidation Loss: -2617.6283\nEpoch 122, Average Training Loss: -2516.1715\nEpoch 123, Average Training Loss: -2514.0440\nEpoch 124, Average Training Loss: -2513.8712\nEpoch 125, Average Training Loss: -2512.9030\nEpoch 126, Average Training Loss: -2514.3275\nEpoch 127, Average Training Loss: -2513.6602\nEpoch 128, Average Training Loss: -2512.9210\nEpoch 129, Average Training Loss: -2514.6141\nEpoch 130, Average Training Loss: -2516.0804\nEpoch 131, Average Training Loss: -2515.3145\nValidation Loss: -2614.3091\nEpoch 132, Average Training Loss: -2516.7284\nEpoch 133, Average Training Loss: -2513.9836\nEpoch 134, Average Training Loss: -2514.9491\nEpoch 135, Average Training Loss: -2514.7791\nEpoch 136, Average Training Loss: -2513.6265\nEpoch 137, Average Training Loss: -2515.6224\nEpoch 138, Average Training Loss: -2515.4695\nEpoch 139, Average Training Loss: -2516.2436\nEpoch 140, Average Training Loss: -2514.2446\nEpoch 141, Average Training Loss: -2513.6308\nValidation Loss: -2618.7615\nEpoch 142, Average Training Loss: -2516.8846\nEpoch 143, Average Training Loss: -2515.6147\nEpoch 144, Average Training Loss: -2518.2091\nEpoch 145, Average Training Loss: -2515.9100\nEpoch 146, Average Training Loss: -2515.7567\nEpoch 147, Average Training Loss: -2516.4160\nEpoch 148, Average Training Loss: -2515.9265\nEpoch 149, Average Training Loss: -2517.0410\nEpoch 150, Average Training Loss: -2516.9174\nEpoch 151, Average Training Loss: -2518.8466\nValidation Loss: -2618.3877\nEpoch 152, Average Training Loss: -2517.9118\nEpoch 153, Average Training Loss: -2517.5789\nEpoch 154, Average Training Loss: -2516.6341\nEpoch 155, Average Training Loss: -2514.6838\nEpoch 156, Average Training Loss: -2516.2738\nEpoch 157, Average Training Loss: -2514.4395\nEpoch 158, Average Training Loss: -2515.9857\nEpoch 159, Average Training Loss: -2515.6484\nEpoch 160, Average Training Loss: -2515.3597\nEpoch 161, Average Training Loss: -2516.6705\nValidation Loss: -2618.2750\nEpoch 162, Average Training Loss: -2517.0529\nEpoch 163, Average Training Loss: -2517.1273\nEpoch 164, Average Training Loss: -2515.6983\nEpoch 165, Average Training Loss: -2515.2524\nEpoch 166, Average Training Loss: -2515.9319\nEpoch 167, Average Training Loss: -2515.7711\nEpoch 168, Average Training Loss: -2516.3080\nEpoch 169, Average Training Loss: -2517.9993\nEpoch 170, Average Training Loss: -2517.5821\nEpoch 171, Average Training Loss: -2517.8313\nValidation Loss: -2619.7416\nEpoch 172, Average Training Loss: -2515.8029\nEpoch 173, Average Training Loss: -2517.5930\nEpoch 174, Average Training Loss: -2515.4525\nEpoch 175, Average Training Loss: -2517.5702\nEpoch 176, Average Training Loss: -2517.3363\nEpoch 177, Average Training Loss: -2516.6031\nEpoch 178, Average Training Loss: -2515.0271\nEpoch 179, Average Training Loss: -2521.0498\nEpoch 180, Average Training Loss: -2518.8543\nEpoch 181, Average Training Loss: -2515.9883\nValidation Loss: -2619.9199\nEpoch 182, Average Training Loss: -2514.9144\nEpoch 183, Average Training Loss: -2519.1730\nEpoch 184, Average Training Loss: -2519.2191\nEpoch 185, Average Training Loss: -2516.2482\nEpoch 186, Average Training Loss: -2516.2381\nEpoch 187, Average Training Loss: -2517.0178\nEpoch 188, Average Training Loss: -2519.7309\nEpoch 189, Average Training Loss: -2517.6599\nEpoch 190, Average Training Loss: -2517.1496\nEpoch 191, Average Training Loss: -2518.9137\nValidation Loss: -2619.0068\nEpoch 192, Average Training Loss: -2518.5868\nEpoch 193, Average Training Loss: -2517.3186\nEpoch 194, Average Training Loss: -2518.9076\nEpoch 195, Average Training Loss: -2518.3919\nEpoch 196, Average Training Loss: -2518.8173\nEpoch 197, Average Training Loss: -2520.2985\nEpoch 198, Average Training Loss: -2516.4059\nEpoch 199, Average Training Loss: -2517.0007\nEpoch 200, Average Training Loss: -2518.6196\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T01:23:27.605265Z","iopub.execute_input":"2024-04-22T01:23:27.605712Z","iopub.status.idle":"2024-04-22T01:23:27.615165Z","shell.execute_reply.started":"2024-04-22T01:23:27.605678Z","shell.execute_reply":"2024-04-22T01:23:27.613865Z"},"trusted":true},"execution_count":7,"outputs":[]}]}